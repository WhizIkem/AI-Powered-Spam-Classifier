{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f44d40",
   "metadata": {},
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33636dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to handle dataframes/tables\n",
    "import numpy as np  # to handle numerical operations\n",
    "import torch # to handle tensor operations\n",
    "from sklearn.model_selection import train_test_split # to split data into training and testing sets\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # to evaluate model performance\n",
    "from xgboost import XGBClassifier # XGBoost classifier\n",
    "from transformers import AutoTokenizer, AutoModel # To use Advanced NLP models and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6295e117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "# 'encoding' parameter is used to handle special characters in the dataset\n",
    "df_raw = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "# display first 5 rows of the dataset\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f4a082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unnecessary columns\n",
    "df_raw = df_raw[['v1', 'v2']]\n",
    "\n",
    "# rename columns for better understanding\n",
    "df_raw.columns = ['label', 'message']\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cdd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe to a new variable for cleaning\n",
    "df_clean = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc438d3",
   "metadata": {},
   "source": [
    "## Advanced NLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d4a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 00:29:27.494488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT + Logistic Regression</th>\n",
       "      <td>0.992825</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.972789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT + XGBoost</th>\n",
       "      <td>0.989238</td>\n",
       "      <td>0.992806</td>\n",
       "      <td>0.926174</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  Precision    Recall  F1-Score\n",
       "BERT + Logistic Regression  0.992825   0.986207  0.959732  0.972789\n",
       "BERT + XGBoost              0.989238   0.992806  0.926174  0.958333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced NLP Techniques for Spam Detection\n",
    "# using BERT embeddings + Logistic Regression and XGBoost\n",
    "\n",
    "# 1. Load pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()   # set to eval mode\n",
    "\n",
    "# 2. Function to get BERT embeddings for one text\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # mean over token dimension -> (hidden_size,)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# 3. Generate BERT embeddings for all messages\n",
    "embeddings = np.array([get_bert_embeddings(msg) for msg in df_clean['message']])\n",
    "\n",
    "# 4. Encode labels (ham=0, spam=1)\n",
    "labels = df_clean['label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "# 5. Train–test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# 6. Helper to evaluate a model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 7. BERT + Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "results[\"BERT + Logistic Regression\"] = eval_model(log_reg, X_test, y_test)\n",
    "\n",
    "# 8. BERT + XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "results[\"BERT + XGBoost\"] = eval_model(xgb_model, X_test, y_test)\n",
    "\n",
    "# 9. Show metrics\n",
    "metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "bert_results_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=metrics_names)\n",
    "bert_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed61664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DistilBERT + Logistic Regression</th>\n",
       "      <td>0.991928</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.953020</td>\n",
       "      <td>0.969283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DistilBERT + XGBoost</th>\n",
       "      <td>0.990135</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.962199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Accuracy  Precision    Recall  F1-Score\n",
       "DistilBERT + Logistic Regression  0.991928   0.986111  0.953020  0.969283\n",
       "DistilBERT + XGBoost              0.990135   0.985915  0.939597  0.962199"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced NLP Techniques for Spam Detection\n",
    "# using DistilBERT embeddings + Logistic Regression and XGBoost \n",
    "\n",
    "# 1. Load pre-trained DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "distilbert_model = AutoModel.from_pretrained(model_name)\n",
    "distilbert_model.eval()   # set to eval mode\n",
    "\n",
    "# 2. Function to get DistilBERT embeddings for one text\n",
    "def get_distilbert_embeddings(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(**inputs)\n",
    "    # mean over token dimension -> (hidden_size,)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# 3. Generate DistilBERT embeddings for all messages\n",
    "embeddings = np.array([get_distilbert_embeddings(msg) for msg in df_clean['message']])\n",
    "\n",
    "# 4. Encode labels (ham=0, spam=1)\n",
    "labels = df_clean['label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "# 5. Train–test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# 6. Helper to evaluate a model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 7. DistilBERT + Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "results[\"DistilBERT + Logistic Regression\"] = eval_model(log_reg, X_test, y_test)\n",
    "\n",
    "# 8. DistilBERT + XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "results[\"DistilBERT + XGBoost\"] = eval_model(xgb_model, X_test, y_test)\n",
    "\n",
    "# 9. Show metrics as DataFrame\n",
    "metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "distilbert_results_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=metrics_names)\n",
    "distilbert_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729c5e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RoBERTa + Logistic Regression</th>\n",
       "      <td>0.992825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.972414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa + XGBoost</th>\n",
       "      <td>0.992825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.972414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Precision    Recall  F1-Score\n",
       "RoBERTa + Logistic Regression  0.992825        1.0  0.946309  0.972414\n",
       "RoBERTa + XGBoost              0.992825        1.0  0.946309  0.972414"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced NLP Techniques for Spam Detection\n",
    "# using RoBERTa embeddings + Logistic Regression and XGBoost\n",
    "\n",
    "# 1. Load pre-trained RoBERTa model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "roberta_model = AutoModel.from_pretrained(model_name)\n",
    "roberta_model.eval()   # set to eval mode\n",
    "\n",
    "# 2. Function to get RoBERTa embeddings for one text\n",
    "def get_roberta_embeddings(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model(**inputs)\n",
    "    # mean over token dimension -> (hidden_size,)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# 3. Generate RoBERTa embeddings for all messages\n",
    "embeddings = np.array([get_roberta_embeddings(msg) for msg in df_clean['message']])\n",
    "\n",
    "# 4. Encode labels (ham=0, spam=1)\n",
    "labels = df_clean['label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "# 5. Train–test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# 6. Helper to evaluate a model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 7. RoBERTa + Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "results[\"RoBERTa + Logistic Regression\"] = eval_model(log_reg, X_test, y_test)\n",
    "\n",
    "# 8. RoBERTa + XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "results[\"RoBERTa + XGBoost\"] = eval_model(xgb_model, X_test, y_test)\n",
    "\n",
    "# 9. Show metrics as DataFrame\n",
    "metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "roberta_results_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=metrics_names)\n",
    "roberta_results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
